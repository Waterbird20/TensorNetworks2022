{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["# Tensor contractions\n",
"Author: <https://cqm.snu.ac.kr Seung-Sup Lee>\n",
"\n",
"Here we will explain how to contract tensors in MATLAB. Consider three tensors \n",
"A, B, and C:\n",
"\n",
"\n",
"\n",
"The legs with the same indices will be contracted. In this tutorial, we will \n",
"treat the tensors as mere numerical arrays in which the directions of tensor \n",
"legs are not important. Thus we will omit the arrows for the rest of this tutorial. \n",
"Of course, when the tensors are in physical context (e.g., bras, kets) or non-Abelian \n",
"symmetries are exploited (to be covered later in the lecture course), the leg \n",
"directions do matter!\n",
"## Initialization\n",
"Clear workspace (clear pre-existing variables to avoid any collision), and \n",
"set the leg dimensions to determine the size of tensors. Leg dimensions are \n",
"also often called bond dimensions when the corresponding legs are from different \n",
"tensors and to be contracted to \"bond\" tensors.\n",
"```Matlab\n",
"clear\n",
"\n",
"% leg dimensions\n",
"d_a = 101; % d_alpha\n",
"d_b = 102; % d_beta\n",
"d_c = 103; % d_gamma\n",
"d_d = 104; % d_delta\n",
"d_m = 105; % d_mu\n",
"```\n",
"Then generate rank-2 tensor **A** (of size d_c-by-d_d) and rank-3 tensors **B** \n",
"(of size d_a-by-d_m-by-d_c) and **C** (of size d_b-by-d_m-by-d_d) with random \n",
"elements.\n",
"```Matlab\n",
"A = rand(d_c,d_d);     % tensor A(gamma,delta)\n",
"B = rand(d_a,d_m,d_c); % tensor B(alpha,mu,gamma)\n",
"C = rand(d_b,d_m,d_d); % tensor C(beta,mu,delta)\n",
"```\n",
"Initiate timers for real/CPU tiume.\n",
"```Matlab\n",
"tobj = tic2;\n",
"## Contract B and C\n",
"Let's contract B and C first. Bring the rank-3 tensor B into a matrix from, \n",
"by permuting its legs and by reshaping, as depicted by the following diagram:\n",
"\n",
"\n",
"\n",
"Here the thick leg means that the associated leg dimension is big, since two \n",
"legs are combined by **reshape**.\n",
"\n",
"B1 = permute(B,[1,3,2]); % B(alpha,mu,gamma) -> B(alpha,gamma,mu)\n",
"B1 = reshape(B1,[d_a*d_c,d_m]); % B(alpha,gamma,mu) -> B(alpha*gamma,mu)\n",
"```\n",
"Treat tensor C similarly.\n",
"```Matlab\n",
"C1 = permute(C,[2,1,3]); % C(beta,mu,delta) -> C(mu,beta,delta)\n",
"C1 = reshape(C1,[d_m,d_b*d_d]); % C(mu,beta,delta) -> C(mu;beta*delta)\n",
"```\n",
"The reshaped **B** and **C** (numerically represented by **B1** and **C1**) are matrices \n",
"(i.e., rank-2 tensors), so the legs can be contracted via matrix multiplication. \n",
"As mentioned in the previous tutorial, MATLAB is very efficient when it performs \n",
"linear algebra operations. Let's contract **B1** and **C1** via their $\\mu$-legs, \n",
"and separate the combined $\\alpha \\otimes \\gamma$- and $\\delta \\otimes \\beta$-legs \n",
"into four legs $\\alpha ,\\beta ,\\gamma ,\\delta$, as below.\n",
"\n",
"\n",
"```Matlab\n",
"BC = B1*C1;% \\sum_{mu} B(alpha*gamma,mu) * C(mu,beta*delta) \n",
"           % = BC(alpha*gamma,beta,delta)\n",
"BC(alpha*gamma,beta*delta) -> BC(alpha,gamma,beta,delta)\n",
"BC = reshape(BC,[d_a,d_c,d_b,d_d]);\n",
"## Contract BC and A\n",
"The remaining tensors look like:\n",
"\n",
"\n",
"\n",
"Bring **BC** into a matrix from by using **permute** and **reshape**.\n",
"\n",
"\n",
"\n",
"% BC(alpha,gamma,beta,delta) -> BC(alpha,beta,gamma,delta)\n",
"BC = permute(BC,[1,3,2,4]);\n",
"BC(alpha,beta,gamma,delta) -> BC(alpha*beta;gamma*delta)\n",
"BC = reshape(BC,[d_a*d_b,d_c*d_d]);\n",
"```\n",
"Then reshape tensor **A** into a vector (though it's treated as a \"thin\" matrix \n",
"in MATLAB) and multiply it with **BC**. By reshaping **ABC** into rank-2 tensor, \n",
"we have rank-2 tensor **ABC**.\n",
"\n",
"\n",
"\n",
"\n",
"```Matlab\n",
"A1 = A(:);   % A(gamma,delta) -> A(gamma*delta)\n",
"\\sum_{gamma,delta} BC(alpha*beta,gamma*delta) * A(gamma*delta) \n",
"      = ABC(alpha,beta)\n",
"ABC1 = BC*A1;              \n",
"ABC1 = reshape(ABC1,[d_a,d_b]);% ABC(alpha*beta) -> ABC(alpha,beta)\n",
"```\n",
"How much time has been taken?\n",
"```Matlab\n",
"toc2(tobj,'-v');\n",
"```\n",
"Usually CPU time lapse is several times larger than real time lapse. It shows \n",
"that MATLAB automatically parallelized computation.\n",
"## Short remark: Why do we use matrix multiplication, instead of for-loops?\n",
"One may ask why we bother with reshaping and permuting tensors. So let's compare \n",
"the computational costs between two approaches. First, below is the part of \n",
"the above code contracting **B** and **C**.\n",
"```Matlab\n",
"% % Scheme 1: Tensor contraction using matrix multiplication\n",
"tobj = tic2;\n",
"\n",
"B1 = permute(B,[1,3,2]); % B(alpha,mu,gamma) -> B(alpha,gamma,mu)\n",
"B1 = reshape(B1,[d_a*d_c,d_m]);% B(alpha,gamma,mu) -> B(alpha*gamma,mu)\n",
"C1 = permute(C,[2,1,3]); % C(beta,mu,delta) -> C(mu,beta,delta)\n",
"C1 = reshape(C1,[d_m,d_b*d_d]);% C(mu,beta,delta) -> C(mu,beta*delta)\n",
"\\sum_{mu} B(alpha*gamma,mu) * C(mu,beta*delta)\n",
"     = BC(alpha*gamma,beta*delta)\n",
"BC = B1*C1;              \n",
"BC(alpha*gamma,beta*delta) -> BC(alpha,gamma,beta,delta)\n",
"BC = reshape(BC,[d_a,d_c,d_b,d_d]);\n",
"\n",
"toc2(tobj,'-v');\n",
"```\n",
"Second, this is a contraction using for-loops. (If it takes too long and your \n",
"laptop suffers, press Ctrl+C or click \"Stop\" (red square button) on the top.)\n",
"```Matlab\n",
"% % Scheme 2: Tensor contraction using for-loops\n",
"tobj = tic2;\n",
"\n",
"% create an 4D-array initialized with zeros\n",
"BC = zeros(d_a,d_c,d_b,d_d);\n",
"for it1 = (1:size(BC,1)) % alpha\n",
"    for it2 = (1:size(BC,2)) % gamma\n",
"        for it3 = (1:size(BC,3)) % beta\n",
"            for it4 = (1:size(BC,4)) % delta\n",
"                for it5 = (1:size(B,2)) % mu\n",
"                    BC(it1,it2,it3,it4) = ...\n",
"                        BC(it1,it2,it3,it4) + ...\n",
"                        B(it1,it5,it2)*C(it3,it5,it4);\n",
"                end\n",
"            end\n",
"        end\n",
"    end\n",
"end\n",
"\n",
"toc2(tobj,'-v');\n",
"```\n",
"We see that the latter scheme takes much longer time (about 100 times)! (*Note:* \n",
"actual ratio of CPU times may depend on many factors, such as the randomness \n",
"in tensor initialization, MATLAB version, CPU architecture, etc.)\n",
"\n",
"In MATLAB, matrix operation is much faster than for-loops, since MATLAB implements \n",
"a state-of-the-art linear algebra algorithm which is better parallelizable, \n",
"for example. Try to avoid use for-loops for matrix or tensor operations as much \n",
"as possible!\n",
"## Exercise (a): First contract A and C, and then contract AC and B\n",
"Try a different order of the tensor contraction. Contract A and C first, then \n",
"contract B, as a diagram below.\n",
"\n",
"\n",
"\n",
"Write a script which implements this way of tensor contraction, and compare \n",
"the computational costs in terms of both real time and CPU time. Which one is \n",
"faster, by which factor?"

   ]
  }
 ],
 "metadata": {
	"language_info": {
	 "name": "python"
	},
	"orig_nbformat": 4
   },
   "nbformat": 4,
   "nbformat_minor": 2
}